{"updatedAt":"2026-02-22T00:55:31.922Z","createdAt":"2026-02-22T00:53:54.893Z","id":"anphyr0Am3ub4IiA","name":"Parallel Processor document Categorizer ANK","description":null,"active":true,"isArchived":false,"nodes":[{"parameters":{"operation":"getAll","tableId":"document_chunks","returnAll":true,"filterType":"string","filterString":"=document_id=eq.{{ $json.documentId }}&order=chunk_index.asc"},"type":"n8n-nodes-base.supabase","typeVersion":1,"position":[-48,-96],"id":"b3e5d380-8b34-4260-9815-25ba1de3b5fa","name":"Get Chunks","credentials":{"supabaseApi":{"id":"FTSugGkqwnuu6irz","name":"SupaStack"}}},{"parameters":{"jsCode":"// Intelligently sample chunks for categorization - OPTIMIZED for 12K tokens\nconst chunks = $input.all().map(item => item.json);\n\n// Get document info directly from Start node\nconst startData = $('Start').first().json;\nconst documentId = startData.documentId;\nconst versionId = startData.versionId;\nconst dimensions = startData.dimensions || [];\nconst dimensionList = startData.dimensionList || '';\n\nif (chunks.length === 0) {\n  console.log(`No chunks found for document ${documentId}`);\n  return [{\n    json: {\n      documentId: documentId,\n      versionId: versionId,\n      error: 'no_chunks_found',\n      content: ''\n    }\n  }];\n}\n\n// Calculate total tokens\nconst totalTokens = chunks.reduce((sum, c) => sum + (c.token_count || 0), 0);\nconst totalChars = chunks.reduce((sum, c) => sum + (c.char_count || 0), 0);\n\nconsole.log(`Document ${documentId}: ${chunks.length} chunks, ~${totalTokens} tokens`);\n\n// OPTIMIZED: Target ~12K tokens for categorization (down from 25K)\nconst MAX_TOKENS = 12000;\nlet sampledContent = '';\nlet sampledTokens = 0;\nlet samplingStrategy = 'full';\n\nif (totalTokens <= MAX_TOKENS) {\n  // Small enough - use all content\n  sampledContent = chunks.map(c => c.content_text || '').join('\\n\\n---\\n\\n');\n  sampledTokens = totalTokens;\n  samplingStrategy = 'full';\n  console.log('Using full document content');\n} else {\n  // Need to sample intelligently\n  samplingStrategy = 'sampled';\n  const chunkCount = chunks.length;\n  \n  // OPTIMIZED Strategy: First chunk, last chunk, 3-5 middle chunks\n  // Aim for ~12K tokens total\n  \n  let selectedChunks = [];\n  let selectedIndices = new Set();\n  \n  // Always include first chunk (intro/context)\n  selectedChunks.push({ index: 0, chunk: chunks[0] });\n  selectedIndices.add(0);\n  \n  // Always include last chunk (conclusions)\n  if (chunkCount > 1) {\n    selectedChunks.push({ index: chunkCount - 1, chunk: chunks[chunkCount - 1] });\n    selectedIndices.add(chunkCount - 1);\n  }\n  \n  // Calculate tokens so far\n  sampledTokens = selectedChunks.reduce((sum, sc) => sum + (sc.chunk.token_count || 0), 0);\n  \n  // Add middle chunks until we hit ~12K tokens\n  const middleStart = 1;\n  const middleEnd = chunkCount - 2;\n  \n  if (middleEnd > middleStart) {\n    // Calculate how many middle chunks we can add\n    const remainingBudget = MAX_TOKENS - sampledTokens;\n    const middleChunks = chunks.slice(middleStart, middleEnd + 1);\n    const avgMiddleTokens = middleChunks.reduce((sum, c) => sum + (c.token_count || 0), 0) / middleChunks.length;\n    \n    // Estimate how many middle chunks we can fit\n    const maxMiddleChunks = Math.floor(remainingBudget / avgMiddleTokens);\n    const middleCount = middleEnd - middleStart + 1;\n    \n    if (maxMiddleChunks >= middleCount) {\n      // Can fit all middle chunks\n      for (let i = middleStart; i <= middleEnd; i++) {\n        if (!selectedIndices.has(i)) {\n          selectedChunks.push({ index: i, chunk: chunks[i] });\n          selectedIndices.add(i);\n        }\n      }\n    } else {\n      // Sample evenly from middle (target 3-5 chunks)\n      const targetMiddle = Math.min(5, maxMiddleChunks);\n      const step = Math.max(1, Math.floor(middleCount / Math.max(1, targetMiddle)));\n      for (let i = middleStart; i <= middleEnd && selectedChunks.length < targetMiddle + 2; i += step) {\n        if (!selectedIndices.has(i)) {\n          selectedChunks.push({ index: i, chunk: chunks[i] });\n          selectedIndices.add(i);\n        }\n      }\n    }\n  }\n  \n  // Sort by original index to maintain document flow\n  selectedChunks.sort((a, b) => a.index - b.index);\n  \n  // Build sampled content with markers\n  const contentParts = [];\n  let lastIndex = -1;\n  \n  for (const sc of selectedChunks) {\n    if (lastIndex !== -1 && sc.index > lastIndex + 1) {\n      const skipped = sc.index - lastIndex - 1;\n      contentParts.push(`\\n\\n[... ${skipped} chunk(s) omitted for brevity ...]\\n\\n`);\n    }\n    contentParts.push(sc.chunk.content_text || '');\n    lastIndex = sc.index;\n  }\n  \n  sampledContent = contentParts.join('\\n\\n---\\n\\n');\n  sampledTokens = selectedChunks.reduce((sum, sc) => sum + (sc.chunk.token_count || 0), 0);\n  \n  console.log(`Sampled ${selectedChunks.length} of ${chunkCount} chunks (~${sampledTokens} tokens)`);\n  console.log(`Selected indices: ${Array.from(selectedIndices).sort((a,b) => a-b).join(', ')}`);\n}\n\nconsole.log(`Dimensions available: ${dimensions.length}`);\nconsole.log(`Sampling strategy: ${samplingStrategy}`);\n\nreturn [{\n  json: {\n    documentId: documentId,\n    versionId: versionId,\n    assessmentId: startData.assessmentId,\n    content: sampledContent,\n    chunkCount: chunks.length,\n    totalTokens: totalTokens,\n    sampledTokens: sampledTokens,\n    samplingStrategy: samplingStrategy,\n    dimensionList: dimensionList,\n    dimensions: dimensions\n  }\n}];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[128,-112],"id":"1eb81b4a-3913-4e93-9700-29fe8af9ba22","name":"Combine Chunks"},{"parameters":{"model":"anthropic/claude-haiku-4.5","options":{}},"type":"@n8n/n8n-nodes-langchain.lmChatOpenRouter","typeVersion":1,"position":[288,96],"id":"71985339-485a-4199-9b3a-c43fd07296b8","name":"Claude Haiku 4.5","credentials":{"openRouterApi":{"id":"DPnbXJEN5OpXBJeM","name":"OpenRouter account"}}},{"parameters":{"promptType":"define","text":"=You are a document categorization specialist. Analyze the following document and determine which assessment dimensions it is relevant to.\n\n## DOCUMENT ID\n{{ $json.documentId }}\n\n## AVAILABLE DIMENSIONS\nThese are the dimensions from the assessment playbook. Only categorize to dimensions where you have >= 60% confidence:\n\n{{ $json.dimensionList }}\n\n## DOCUMENT CONTENT\n{{ $json.content }}\n\n---\n\n## YOUR TASK\n\n1. Read the document carefully\n2. Identify which dimensions this document provides evidence for\n3. For EACH relevant dimension (confidence >= 60%):\n   - Assign a confidence score (0.6 to 1.0)\n   - Explain your reasoning\n   - Extract key findings relevant to that dimension\n   - Extract any quantitative metrics found\n   - Include up to 3 short relevant quotes (max 100 chars each)\n\n4. Provide a brief document summary\n5. Classify the document type\n\n## IMPORTANT NOTES\n- A document can be relevant to MULTIPLE dimensions\n- Only include dimensions where you have >= 60% confidence\n- Be specific with key findings - they will be used by dimension analysis agents\n- Metrics are valuable - extract any numbers, percentages, dollar amounts\n- Keep quotes short and impactful\n\n---\n\n## RESPONSE FORMAT (MANDATORY)\n\nYou MUST respond with ONLY a valid JSON object. No markdown, no code fences, no explanation text before or after. Just the raw JSON.\n\nReturn this EXACT structure:\n\n{\n  \"document_id\": \"<the document ID being categorized>\",\n  \"relevant_dimensions\": [\n    {\n      \"dimension_key\": \"<dimension key from the provided list>\",\n      \"dimension_name\": \"<human-readable dimension name>\",\n      \"confidence_score\": <number 0.6-1.0>,\n      \"reasoning\": \"<brief explanation of why this document is relevant to this dimension>\",\n      \"key_findings\": [\"<finding 1>\", \"<finding 2>\"],\n      \"metrics_found\": [\n        {\n          \"metric_name\": \"<name>\",\n          \"value\": \"<value>\",\n          \"context\": \"<context>\"\n        }\n      ],\n      \"relevant_quotes\": [\"<quote 1 max 100 chars>\", \"<quote 2>\"]\n    }\n  ],\n  \"document_summary\": \"<brief summary of the document overall content and purpose>\",\n  \"document_type\": \"<e.g. financial_report, partnership_agreement, strategy_doc, process_doc, metrics_dashboard>\"\n}\n\nCRITICAL RULES:\n- Output ONLY the JSON object, nothing else\n- All required fields: document_id, relevant_dimensions, document_summary, document_type\n- Each dimension requires: dimension_key, dimension_name, confidence_score, reasoning, key_findings\n- confidence_score must be between 0.6 and 1.0\n- relevant_quotes max 3 items, each max 100 characters\n- Only include dimensions where confidence >= 0.6\n\n**Begin your analysis now. Output ONLY the JSON.**","options":{}},"type":"@n8n/n8n-nodes-langchain.agent","typeVersion":1.8,"position":[304,-144],"id":"c4f19e73-84b5-4e07-a176-e50cfd4d203c","name":"Categorization Agent"},{"parameters":{"jsCode":"// Parse AI output and prepare for database insertion\nconst startData = $('Start').first().json;\nconst combineData = $('Combine Chunks').first().json;\n\nconst documentId = startData.documentId;\nconst versionId = startData.versionId;\n\nlet aiOutput;\ntry {\n  const rawOutput = $input.first().json;\n  \n  if (rawOutput.output) {\n    aiOutput = typeof rawOutput.output === 'string' \n      ? JSON.parse(rawOutput.output) \n      : rawOutput.output;\n  } else {\n    aiOutput = rawOutput;\n  }\n} catch (e) {\n  console.log(`Failed to parse AI output: ${e.message}`);\n  return [{\n    json: {\n      _skipInsert: true,\n      error: 'parse_failed',\n      documentId: documentId,\n      message: e.message\n    }\n  }];\n}\n\nconst relevantDimensions = aiOutput.relevant_dimensions || [];\n\nconsole.log(`Document ${documentId}: ${relevantDimensions.length} dimensions found`);\n\nif (relevantDimensions.length === 0) {\n  console.log('No relevant dimensions found - skipping insert');\n  return [{\n    json: {\n      _skipInsert: true,\n      documentId: documentId,\n      versionId: versionId,\n      status: 'no_dimensions',\n      documentSummary: aiOutput.document_summary,\n      documentType: aiOutput.document_type\n    }\n  }];\n}\n\n// Create assignment records for each dimension\nconst assignments = relevantDimensions.map(dim => ({\n  json: {\n    _skipInsert: false,\n    document_id: documentId,\n    version_id: versionId,\n    dimension_key: dim.dimension_key,\n    dimension_name: dim.dimension_name,\n    confidence_score: dim.confidence_score,\n    assignment_reasoning: dim.reasoning,\n    extracted_insights: JSON.stringify({\n      key_findings: dim.key_findings || [],\n      metrics_found: dim.metrics_found || [],\n      evidence: dim.relevant_quotes || []\n    }),\n    relevant_quotes: dim.relevant_quotes || [],\n    insight_count: (dim.key_findings || []).length + (dim.metrics_found || []).length,\n    assigned_by_model: 'claude-sonnet-4'\n  }\n}));\n\nconsole.log(`Created ${assignments.length} dimension assignments`);\n\nreturn assignments;"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[720,-112],"id":"20e72d0d-f6f8-4517-8d2c-4e87cdab24fd","name":"Parse AI Output"},{"parameters":{"tableId":"document_dimension_assignments","fieldsUi":{"fieldValues":[{"fieldId":"assessment_id","fieldValue":"={{ $('Start').first().json.assessmentId }}"},{"fieldId":"document_id","fieldValue":"={{ $json.document_id.toString() }}"},{"fieldId":"version_id","fieldValue":"={{ $json.version_id }}"},{"fieldId":"dimension_key","fieldValue":"={{ $json.dimension_key }}"},{"fieldId":"dimension_name","fieldValue":"={{ $json.dimension_name }}"},{"fieldId":"confidence_score","fieldValue":"={{ $json.confidence_score }}"},{"fieldId":"assignment_reasoning","fieldValue":"={{ $json.assignment_reasoning }}"},{"fieldId":"extracted_insights","fieldValue":"={{ JSON.stringify($json.extracted_insights) }}"},{"fieldId":"relevant_quotes","fieldValue":"={{ $json.relevant_quotes }}"}]}},"type":"n8n-nodes-base.supabase","typeVersion":1,"position":[912,-112],"id":"b4b5ed1b-7663-4fc4-b319-10613d849c96","name":"Insert Assignments","credentials":{"supabaseApi":{"id":"FTSugGkqwnuu6irz","name":"SupaStack"}}},{"parameters":{"jsCode":"// Return results instead of storing in staticData\nconst assignments = $input.all();\nconst startData = $('Start').first().json;\n\n// Count non-skipped assignments\nconst validAssignments = assignments.filter(a => !a.json._skipInsert);\n\nconst result = {\n  documentId: startData.documentId,\n  versionId: startData.versionId,\n  assessmentId: startData.assessmentId,\n  dimensionsAssigned: validAssignments.length,\n  status: validAssignments.length > 0 ? 'categorized' : 'no_dimensions',\n  assignments: validAssignments.map(a => ({\n    dimension_key: a.json.dimension_key,\n    confidence_score: a.json.confidence_score\n  }))\n};\n\nconsole.log(`Document ${result.documentId}: ${result.dimensionsAssigned} dimensions assigned`);\n\nreturn [{ json: result }];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[1184,-112],"id":"374e499f-6d63-4fa7-bac1-f4305572c79f","name":"Track Progress"},{"parameters":{"inputSource":"passthrough"},"id":"79d0959b-e02c-410c-b243-7bcec3a87fe1","typeVersion":1.1,"name":"Start","type":"n8n-nodes-base.executeWorkflowTrigger","position":[-288,-96]},{"parameters":{"jsCode":"// Parse JSON from agent raw text output\nconst input = $input.first().json;\n\n// If already an object with our expected fields, pass through\nif (input.document_id !== undefined && input.relevant_dimensions !== undefined) {\n  return [{ json: input }];\n}\n\n// Get the text output from the agent\nconst text = input.output || input.text || (typeof input === 'string' ? input : JSON.stringify(input));\n\n// Try to extract JSON from the text\nlet parsed = null;\ntry {\n  parsed = JSON.parse(text);\n} catch (e) {\n  // Try to find JSON block in markdown code fences\n  const jsonMatch = text.match(/```(?:json)?\\s*([\\s\\S]*?)```/);\n  if (jsonMatch) {\n    try {\n      parsed = JSON.parse(jsonMatch[1].trim());\n    } catch (e2) {\n      // fall through\n    }\n  }\n  if (!parsed) {\n    // Try to find first { to last }\n    const start = text.indexOf('{');\n    const end = text.lastIndexOf('}');\n    if (start !== -1 && end !== -1 && end > start) {\n      try {\n        parsed = JSON.parse(text.substring(start, end + 1));\n      } catch (e3) {\n        return [{ json: { error: 'Failed to parse JSON from agent output', raw_output: text.substring(0, 2000) } }];\n      }\n    } else {\n      return [{ json: { error: 'No JSON found in agent output', raw_output: text.substring(0, 2000) } }];\n    }\n  }\n}\n\nreturn [{ json: parsed }];"},"id":"fba141e3-bde7-455f-b019-9facc9005b2e","name":"Parse Agent JSON","type":"n8n-nodes-base.code","typeVersion":2,"position":[560,-144]}],"connections":{"Get Chunks":{"main":[[{"node":"Combine Chunks","type":"main","index":0}]]},"Combine Chunks":{"main":[[{"node":"Categorization Agent","type":"main","index":0}]]},"Claude Haiku 4.5":{"ai_languageModel":[[{"node":"Categorization Agent","type":"ai_languageModel","index":0}]]},"Parse AI Output":{"main":[[{"node":"Insert Assignments","type":"main","index":0}]]},"Insert Assignments":{"main":[[{"node":"Track Progress","type":"main","index":0}]]},"Start":{"main":[[{"node":"Get Chunks","type":"main","index":0}]]},"Categorization Agent":{"main":[[{"node":"Parse Agent JSON","type":"main","index":0}]]},"Parse Agent JSON":{"main":[[{"node":"Parse AI Output","type":"main","index":0}]]}},"settings":{"executionOrder":"v1","availableInMCP":false,"callerPolicy":"workflowsFromSameOwner"},"staticData":{"global":{"processedDocuments":[{"documentId":"ac69b659-a846-4ed0-b101-2391c38827ca","dimensionsAssigned":6},{"documentId":"ac69b659-a846-4ed0-b101-2391c38827ca","dimensionsAssigned":7},{"documentId":"206f6eaa-7bee-48ca-ab0b-eeba7899b041","dimensionsAssigned":5},{"documentId":"659a01e6-80be-4e8a-af91-2def4d736d90","dimensionsAssigned":7},{"documentId":"77c8c376-4690-4c0b-8132-8bfb314dff6a","dimensionsAssigned":7},{"documentId":"7fc22f39-bf1d-440f-a868-c8b483254e55","dimensionsAssigned":5},{"documentId":"0fb0d363-093a-49aa-ac26-c3b2b7b40d29","dimensionsAssigned":5},{"documentId":"cfb682c0-2f82-4788-b30e-556b8feadf1a","dimensionsAssigned":7},{"documentId":"60dc1dc8-7d29-4623-8fef-a607283dd386","dimensionsAssigned":5}]}},"meta":null,"pinData":{"Start":[{"json":{"documentId":"d007d9ac-dbfc-41d1-9421-f4d09413ff2e","versionId":"75e813b5-98f7-4afd-babd-d288fdfdb0d1","assessmentId":"a0dc498d-9411-49f0-9359-c5989f4acabc","dimensions":[{"key":"people_org","name":"People & Organizational Readiness","weight":0.17,"description":"Assessment of people & organizational readiness capabilities and maturity for B2B Global Sourcing & Procurement Services"},{"key":"data_analytics","name":"Data Infrastructure & Analytics","weight":0.17,"description":"Assessment of data infrastructure & analytics capabilities and maturity for B2B Global Sourcing & Procurement Services"},{"key":"technology","name":"Technology Stack & Integration","weight":0.17,"description":"Assessment of technology stack & integration capabilities and maturity for B2B Global Sourcing & Procurement Services"},{"key":"commercial","name":"Commercial Strategy & Market Approach","weight":0.17,"description":"Assessment of commercial strategy & market approach capabilities and maturity for B2B Global Sourcing & Procurement Services"},{"key":"operations","name":"Operations & Process Excellence","weight":0.16,"description":"Assessment of operations & process excellence capabilities and maturity for B2B Global Sourcing & Procurement Services"},{"key":"financial","name":"Financial Operations & Business Model","weight":0.16,"description":"Assessment of financial operations & business model capabilities and maturity for B2B Global Sourcing & Procurement Services"}],"dimensionList":"- people_org: People & Organizational Readiness - Assessment of people & organizational readiness capabilities and maturity for B2B Global Sourcing & Procurement Services\n- data_analytics: Data Infrastructure & Analytics - Assessment of data infrastructure & analytics capabilities and maturity for B2B Global Sourcing & Procurement Services\n- technology: Technology Stack & Integration - Assessment of technology stack & integration capabilities and maturity for B2B Global Sourcing & Procurement Services\n- commercial: Commercial Strategy & Market Approach - Assessment of commercial strategy & market approach capabilities and maturity for B2B Global Sourcing & Procurement Services\n- operations: Operations & Process Excellence - Assessment of operations & process excellence capabilities and maturity for B2B Global Sourcing & Procurement Services\n- financial: Financial Operations & Business Model - Assessment of financial operations & business model capabilities and maturity for B2B Global Sourcing & Procurement Services"},"pairedItem":{"item":0}}]},"versionId":"b78ecba3-2d5f-4e5f-9035-e0db9b9cd61c","activeVersionId":"b78ecba3-2d5f-4e5f-9035-e0db9b9cd61c","versionCounter":77,"triggerCount":0,"shared":[{"updatedAt":"2026-02-22T00:53:54.895Z","createdAt":"2026-02-22T00:53:54.895Z","role":"workflow:owner","workflowId":"anphyr0Am3ub4IiA","projectId":"DVYglwqlfMJuM9zT","project":{"updatedAt":"2025-10-05T18:15:43.115Z","createdAt":"2025-08-13T20:40:27.041Z","id":"DVYglwqlfMJuM9zT","name":"Laksh Agrawal <laksh@supastack.ai>","type":"personal","icon":null,"description":null,"creatorId":"ec164287-6cde-4606-b0c4-9d4cd30c3b98","projectRelations":[{"updatedAt":"2025-08-13T20:40:27.041Z","createdAt":"2025-08-13T20:40:27.041Z","userId":"ec164287-6cde-4606-b0c4-9d4cd30c3b98","projectId":"DVYglwqlfMJuM9zT","user":{"updatedAt":"2026-02-22T00:41:30.000Z","createdAt":"2025-08-13T20:40:26.693Z","id":"ec164287-6cde-4606-b0c4-9d4cd30c3b98","email":"laksh@supastack.ai","firstName":"Laksh","lastName":"Agrawal","personalizationAnswers":{"version":"v4","personalization_survey_submitted_at":"2025-08-13T20:49:45.872Z","personalization_survey_n8n_version":"1.106.3","companyIndustryExtended":[],"companyType":"other"},"settings":{"userActivated":true,"easyAIWorkflowOnboarded":true,"firstSuccessfulWorkflowId":"omMzGXpD7rLUfAFm","userActivatedAt":1755124587490,"npsSurvey":{"responded":true,"lastShownAt":1759741367883}},"disabled":false,"mfaEnabled":false,"lastActiveAt":"2026-02-21","isPending":false}}]}}],"tags":[],"activeVersion":{"updatedAt":"2026-02-22T00:58:25.000Z","createdAt":"2026-02-22T00:53:54.896Z","versionId":"b78ecba3-2d5f-4e5f-9035-e0db9b9cd61c","workflowId":"anphyr0Am3ub4IiA","nodes":[{"parameters":{"operation":"getAll","tableId":"document_chunks","returnAll":true,"filterType":"string","filterString":"=document_id=eq.{{ $json.documentId }}&order=chunk_index.asc"},"type":"n8n-nodes-base.supabase","typeVersion":1,"position":[-48,-96],"id":"b3e5d380-8b34-4260-9815-25ba1de3b5fa","name":"Get Chunks","credentials":{"supabaseApi":{"id":"FTSugGkqwnuu6irz","name":"SupaStack"}}},{"parameters":{"jsCode":"// Intelligently sample chunks for categorization - OPTIMIZED for 12K tokens\nconst chunks = $input.all().map(item => item.json);\n\n// Get document info directly from Start node\nconst startData = $('Start').first().json;\nconst documentId = startData.documentId;\nconst versionId = startData.versionId;\nconst dimensions = startData.dimensions || [];\nconst dimensionList = startData.dimensionList || '';\n\nif (chunks.length === 0) {\n  console.log(`No chunks found for document ${documentId}`);\n  return [{\n    json: {\n      documentId: documentId,\n      versionId: versionId,\n      error: 'no_chunks_found',\n      content: ''\n    }\n  }];\n}\n\n// Calculate total tokens\nconst totalTokens = chunks.reduce((sum, c) => sum + (c.token_count || 0), 0);\nconst totalChars = chunks.reduce((sum, c) => sum + (c.char_count || 0), 0);\n\nconsole.log(`Document ${documentId}: ${chunks.length} chunks, ~${totalTokens} tokens`);\n\n// OPTIMIZED: Target ~12K tokens for categorization (down from 25K)\nconst MAX_TOKENS = 12000;\nlet sampledContent = '';\nlet sampledTokens = 0;\nlet samplingStrategy = 'full';\n\nif (totalTokens <= MAX_TOKENS) {\n  // Small enough - use all content\n  sampledContent = chunks.map(c => c.content_text || '').join('\\n\\n---\\n\\n');\n  sampledTokens = totalTokens;\n  samplingStrategy = 'full';\n  console.log('Using full document content');\n} else {\n  // Need to sample intelligently\n  samplingStrategy = 'sampled';\n  const chunkCount = chunks.length;\n  \n  // OPTIMIZED Strategy: First chunk, last chunk, 3-5 middle chunks\n  // Aim for ~12K tokens total\n  \n  let selectedChunks = [];\n  let selectedIndices = new Set();\n  \n  // Always include first chunk (intro/context)\n  selectedChunks.push({ index: 0, chunk: chunks[0] });\n  selectedIndices.add(0);\n  \n  // Always include last chunk (conclusions)\n  if (chunkCount > 1) {\n    selectedChunks.push({ index: chunkCount - 1, chunk: chunks[chunkCount - 1] });\n    selectedIndices.add(chunkCount - 1);\n  }\n  \n  // Calculate tokens so far\n  sampledTokens = selectedChunks.reduce((sum, sc) => sum + (sc.chunk.token_count || 0), 0);\n  \n  // Add middle chunks until we hit ~12K tokens\n  const middleStart = 1;\n  const middleEnd = chunkCount - 2;\n  \n  if (middleEnd > middleStart) {\n    // Calculate how many middle chunks we can add\n    const remainingBudget = MAX_TOKENS - sampledTokens;\n    const middleChunks = chunks.slice(middleStart, middleEnd + 1);\n    const avgMiddleTokens = middleChunks.reduce((sum, c) => sum + (c.token_count || 0), 0) / middleChunks.length;\n    \n    // Estimate how many middle chunks we can fit\n    const maxMiddleChunks = Math.floor(remainingBudget / avgMiddleTokens);\n    const middleCount = middleEnd - middleStart + 1;\n    \n    if (maxMiddleChunks >= middleCount) {\n      // Can fit all middle chunks\n      for (let i = middleStart; i <= middleEnd; i++) {\n        if (!selectedIndices.has(i)) {\n          selectedChunks.push({ index: i, chunk: chunks[i] });\n          selectedIndices.add(i);\n        }\n      }\n    } else {\n      // Sample evenly from middle (target 3-5 chunks)\n      const targetMiddle = Math.min(5, maxMiddleChunks);\n      const step = Math.max(1, Math.floor(middleCount / Math.max(1, targetMiddle)));\n      for (let i = middleStart; i <= middleEnd && selectedChunks.length < targetMiddle + 2; i += step) {\n        if (!selectedIndices.has(i)) {\n          selectedChunks.push({ index: i, chunk: chunks[i] });\n          selectedIndices.add(i);\n        }\n      }\n    }\n  }\n  \n  // Sort by original index to maintain document flow\n  selectedChunks.sort((a, b) => a.index - b.index);\n  \n  // Build sampled content with markers\n  const contentParts = [];\n  let lastIndex = -1;\n  \n  for (const sc of selectedChunks) {\n    if (lastIndex !== -1 && sc.index > lastIndex + 1) {\n      const skipped = sc.index - lastIndex - 1;\n      contentParts.push(`\\n\\n[... ${skipped} chunk(s) omitted for brevity ...]\\n\\n`);\n    }\n    contentParts.push(sc.chunk.content_text || '');\n    lastIndex = sc.index;\n  }\n  \n  sampledContent = contentParts.join('\\n\\n---\\n\\n');\n  sampledTokens = selectedChunks.reduce((sum, sc) => sum + (sc.chunk.token_count || 0), 0);\n  \n  console.log(`Sampled ${selectedChunks.length} of ${chunkCount} chunks (~${sampledTokens} tokens)`);\n  console.log(`Selected indices: ${Array.from(selectedIndices).sort((a,b) => a-b).join(', ')}`);\n}\n\nconsole.log(`Dimensions available: ${dimensions.length}`);\nconsole.log(`Sampling strategy: ${samplingStrategy}`);\n\nreturn [{\n  json: {\n    documentId: documentId,\n    versionId: versionId,\n    assessmentId: startData.assessmentId,\n    content: sampledContent,\n    chunkCount: chunks.length,\n    totalTokens: totalTokens,\n    sampledTokens: sampledTokens,\n    samplingStrategy: samplingStrategy,\n    dimensionList: dimensionList,\n    dimensions: dimensions\n  }\n}];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[128,-112],"id":"1eb81b4a-3913-4e93-9700-29fe8af9ba22","name":"Combine Chunks"},{"parameters":{"model":"anthropic/claude-haiku-4.5","options":{}},"type":"@n8n/n8n-nodes-langchain.lmChatOpenRouter","typeVersion":1,"position":[288,96],"id":"71985339-485a-4199-9b3a-c43fd07296b8","name":"Claude Haiku 4.5","credentials":{"openRouterApi":{"id":"DPnbXJEN5OpXBJeM","name":"OpenRouter account"}}},{"parameters":{"promptType":"define","text":"=You are a document categorization specialist. Analyze the following document and determine which assessment dimensions it is relevant to.\n\n## DOCUMENT ID\n{{ $json.documentId }}\n\n## AVAILABLE DIMENSIONS\nThese are the dimensions from the assessment playbook. Only categorize to dimensions where you have >= 60% confidence:\n\n{{ $json.dimensionList }}\n\n## DOCUMENT CONTENT\n{{ $json.content }}\n\n---\n\n## YOUR TASK\n\n1. Read the document carefully\n2. Identify which dimensions this document provides evidence for\n3. For EACH relevant dimension (confidence >= 60%):\n   - Assign a confidence score (0.6 to 1.0)\n   - Explain your reasoning\n   - Extract key findings relevant to that dimension\n   - Extract any quantitative metrics found\n   - Include up to 3 short relevant quotes (max 100 chars each)\n\n4. Provide a brief document summary\n5. Classify the document type\n\n## IMPORTANT NOTES\n- A document can be relevant to MULTIPLE dimensions\n- Only include dimensions where you have >= 60% confidence\n- Be specific with key findings - they will be used by dimension analysis agents\n- Metrics are valuable - extract any numbers, percentages, dollar amounts\n- Keep quotes short and impactful\n\n---\n\n## RESPONSE FORMAT (MANDATORY)\n\nYou MUST respond with ONLY a valid JSON object. No markdown, no code fences, no explanation text before or after. Just the raw JSON.\n\nReturn this EXACT structure:\n\n{\n  \"document_id\": \"<the document ID being categorized>\",\n  \"relevant_dimensions\": [\n    {\n      \"dimension_key\": \"<dimension key from the provided list>\",\n      \"dimension_name\": \"<human-readable dimension name>\",\n      \"confidence_score\": <number 0.6-1.0>,\n      \"reasoning\": \"<brief explanation of why this document is relevant to this dimension>\",\n      \"key_findings\": [\"<finding 1>\", \"<finding 2>\"],\n      \"metrics_found\": [\n        {\n          \"metric_name\": \"<name>\",\n          \"value\": \"<value>\",\n          \"context\": \"<context>\"\n        }\n      ],\n      \"relevant_quotes\": [\"<quote 1 max 100 chars>\", \"<quote 2>\"]\n    }\n  ],\n  \"document_summary\": \"<brief summary of the document overall content and purpose>\",\n  \"document_type\": \"<e.g. financial_report, partnership_agreement, strategy_doc, process_doc, metrics_dashboard>\"\n}\n\nCRITICAL RULES:\n- Output ONLY the JSON object, nothing else\n- All required fields: document_id, relevant_dimensions, document_summary, document_type\n- Each dimension requires: dimension_key, dimension_name, confidence_score, reasoning, key_findings\n- confidence_score must be between 0.6 and 1.0\n- relevant_quotes max 3 items, each max 100 characters\n- Only include dimensions where confidence >= 0.6\n\n**Begin your analysis now. Output ONLY the JSON.**","options":{}},"type":"@n8n/n8n-nodes-langchain.agent","typeVersion":1.8,"position":[304,-144],"id":"c4f19e73-84b5-4e07-a176-e50cfd4d203c","name":"Categorization Agent"},{"parameters":{"jsCode":"// Parse AI output and prepare for database insertion\nconst startData = $('Start').first().json;\nconst combineData = $('Combine Chunks').first().json;\n\nconst documentId = startData.documentId;\nconst versionId = startData.versionId;\n\nlet aiOutput;\ntry {\n  const rawOutput = $input.first().json;\n  \n  if (rawOutput.output) {\n    aiOutput = typeof rawOutput.output === 'string' \n      ? JSON.parse(rawOutput.output) \n      : rawOutput.output;\n  } else {\n    aiOutput = rawOutput;\n  }\n} catch (e) {\n  console.log(`Failed to parse AI output: ${e.message}`);\n  return [{\n    json: {\n      _skipInsert: true,\n      error: 'parse_failed',\n      documentId: documentId,\n      message: e.message\n    }\n  }];\n}\n\nconst relevantDimensions = aiOutput.relevant_dimensions || [];\n\nconsole.log(`Document ${documentId}: ${relevantDimensions.length} dimensions found`);\n\nif (relevantDimensions.length === 0) {\n  console.log('No relevant dimensions found - skipping insert');\n  return [{\n    json: {\n      _skipInsert: true,\n      documentId: documentId,\n      versionId: versionId,\n      status: 'no_dimensions',\n      documentSummary: aiOutput.document_summary,\n      documentType: aiOutput.document_type\n    }\n  }];\n}\n\n// Create assignment records for each dimension\nconst assignments = relevantDimensions.map(dim => ({\n  json: {\n    _skipInsert: false,\n    document_id: documentId,\n    version_id: versionId,\n    dimension_key: dim.dimension_key,\n    dimension_name: dim.dimension_name,\n    confidence_score: dim.confidence_score,\n    assignment_reasoning: dim.reasoning,\n    extracted_insights: JSON.stringify({\n      key_findings: dim.key_findings || [],\n      metrics_found: dim.metrics_found || [],\n      evidence: dim.relevant_quotes || []\n    }),\n    relevant_quotes: dim.relevant_quotes || [],\n    insight_count: (dim.key_findings || []).length + (dim.metrics_found || []).length,\n    assigned_by_model: 'claude-sonnet-4'\n  }\n}));\n\nconsole.log(`Created ${assignments.length} dimension assignments`);\n\nreturn assignments;"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[720,-112],"id":"20e72d0d-f6f8-4517-8d2c-4e87cdab24fd","name":"Parse AI Output"},{"parameters":{"tableId":"document_dimension_assignments","fieldsUi":{"fieldValues":[{"fieldId":"assessment_id","fieldValue":"={{ $('Start').first().json.assessmentId }}"},{"fieldId":"document_id","fieldValue":"={{ $json.document_id.toString() }}"},{"fieldId":"version_id","fieldValue":"={{ $json.version_id }}"},{"fieldId":"dimension_key","fieldValue":"={{ $json.dimension_key }}"},{"fieldId":"dimension_name","fieldValue":"={{ $json.dimension_name }}"},{"fieldId":"confidence_score","fieldValue":"={{ $json.confidence_score }}"},{"fieldId":"assignment_reasoning","fieldValue":"={{ $json.assignment_reasoning }}"},{"fieldId":"extracted_insights","fieldValue":"={{ JSON.stringify($json.extracted_insights) }}"},{"fieldId":"relevant_quotes","fieldValue":"={{ $json.relevant_quotes }}"}]}},"type":"n8n-nodes-base.supabase","typeVersion":1,"position":[912,-112],"id":"b4b5ed1b-7663-4fc4-b319-10613d849c96","name":"Insert Assignments","credentials":{"supabaseApi":{"id":"FTSugGkqwnuu6irz","name":"SupaStack"}}},{"parameters":{"jsCode":"// Return results instead of storing in staticData\nconst assignments = $input.all();\nconst startData = $('Start').first().json;\n\n// Count non-skipped assignments\nconst validAssignments = assignments.filter(a => !a.json._skipInsert);\n\nconst result = {\n  documentId: startData.documentId,\n  versionId: startData.versionId,\n  assessmentId: startData.assessmentId,\n  dimensionsAssigned: validAssignments.length,\n  status: validAssignments.length > 0 ? 'categorized' : 'no_dimensions',\n  assignments: validAssignments.map(a => ({\n    dimension_key: a.json.dimension_key,\n    confidence_score: a.json.confidence_score\n  }))\n};\n\nconsole.log(`Document ${result.documentId}: ${result.dimensionsAssigned} dimensions assigned`);\n\nreturn [{ json: result }];"},"type":"n8n-nodes-base.code","typeVersion":2,"position":[1184,-112],"id":"374e499f-6d63-4fa7-bac1-f4305572c79f","name":"Track Progress"},{"parameters":{"inputSource":"passthrough"},"id":"79d0959b-e02c-410c-b243-7bcec3a87fe1","typeVersion":1.1,"name":"Start","type":"n8n-nodes-base.executeWorkflowTrigger","position":[-288,-96]},{"parameters":{"jsCode":"// Parse JSON from agent raw text output\nconst input = $input.first().json;\n\n// If already an object with our expected fields, pass through\nif (input.document_id !== undefined && input.relevant_dimensions !== undefined) {\n  return [{ json: input }];\n}\n\n// Get the text output from the agent\nconst text = input.output || input.text || (typeof input === 'string' ? input : JSON.stringify(input));\n\n// Try to extract JSON from the text\nlet parsed = null;\ntry {\n  parsed = JSON.parse(text);\n} catch (e) {\n  // Try to find JSON block in markdown code fences\n  const jsonMatch = text.match(/```(?:json)?\\s*([\\s\\S]*?)```/);\n  if (jsonMatch) {\n    try {\n      parsed = JSON.parse(jsonMatch[1].trim());\n    } catch (e2) {\n      // fall through\n    }\n  }\n  if (!parsed) {\n    // Try to find first { to last }\n    const start = text.indexOf('{');\n    const end = text.lastIndexOf('}');\n    if (start !== -1 && end !== -1 && end > start) {\n      try {\n        parsed = JSON.parse(text.substring(start, end + 1));\n      } catch (e3) {\n        return [{ json: { error: 'Failed to parse JSON from agent output', raw_output: text.substring(0, 2000) } }];\n      }\n    } else {\n      return [{ json: { error: 'No JSON found in agent output', raw_output: text.substring(0, 2000) } }];\n    }\n  }\n}\n\nreturn [{ json: parsed }];"},"id":"fba141e3-bde7-455f-b019-9facc9005b2e","name":"Parse Agent JSON","type":"n8n-nodes-base.code","typeVersion":2,"position":[560,-144]}],"connections":{"Get Chunks":{"main":[[{"node":"Combine Chunks","type":"main","index":0}]]},"Combine Chunks":{"main":[[{"node":"Categorization Agent","type":"main","index":0}]]},"Claude Haiku 4.5":{"ai_languageModel":[[{"node":"Categorization Agent","type":"ai_languageModel","index":0}]]},"Parse AI Output":{"main":[[{"node":"Insert Assignments","type":"main","index":0}]]},"Insert Assignments":{"main":[[{"node":"Track Progress","type":"main","index":0}]]},"Start":{"main":[[{"node":"Get Chunks","type":"main","index":0}]]},"Categorization Agent":{"main":[[{"node":"Parse Agent JSON","type":"main","index":0}]]},"Parse Agent JSON":{"main":[[{"node":"Parse AI Output","type":"main","index":0}]]}},"authors":"Laksh Agrawal","name":"Version b78ecba3","description":"","autosaved":false,"workflowPublishHistory":[{"createdAt":"2026-02-22T00:58:25.231Z","id":267,"workflowId":"anphyr0Am3ub4IiA","versionId":"b78ecba3-2d5f-4e5f-9035-e0db9b9cd61c","event":"activated","userId":"ec164287-6cde-4606-b0c4-9d4cd30c3b98"}]}}